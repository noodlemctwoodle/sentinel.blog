name: Pull Proton VPN Server Data with Geolocation

on:
  workflow_dispatch:
  schedule:
    # Run daily at midnight UTC
    - cron: "0 0 * * *"

jobs:
  pull-protonvpn-data:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Create directory if not exists
        run: mkdir -p ExternalData
      
      - name: Download MaxMind GeoLite2 Database
        run: |
          echo "Downloading MaxMind GeoLite2 database..."
          
          # Method 1: Try direct download from MaxMind (requires free account)
          if [ -n "${{ secrets.MAXMIND_LICENSE_KEY }}" ]; then
            echo "Using MaxMind license key..."
            wget -O GeoLite2-City.tar.gz "https://download.maxmind.com/app/geoip_download?edition_id=GeoLite2-City&license_key=${{ secrets.MAXMIND_LICENSE_KEY }}&suffix=tar.gz"
            if [ -f GeoLite2-City.tar.gz ] && [ -s GeoLite2-City.tar.gz ]; then
              tar -xzf GeoLite2-City.tar.gz
              find . -name "GeoLite2-City.mmdb" -exec mv {} . \;
              echo "✓ Downloaded official MaxMind GeoLite2 database"
            else
              echo "Failed to download from MaxMind, trying alternative..."
            fi
          fi
          
          # Method 2: Alternative source (GitHub mirror)
          if [ ! -f GeoLite2-City.mmdb ]; then
            echo "Trying alternative MaxMind source..."
            wget -O GeoLite2-City.mmdb "https://github.com/P3TERX/GeoLite.mmdb/releases/latest/download/GeoLite2-City.mmdb" || true
          fi
          
          # Method 3: Fallback to CSV format
          if [ ! -f GeoLite2-City.mmdb ]; then
            echo "Downloading IP2Location as fallback..."
            wget -O ip2location.csv "https://download.ip2location.com/lite/IP2LOCATION-LITE-DB5.CSV" || true
            if [ -f ip2location.csv ] && [ -s ip2location.csv ]; then
              echo "✓ Downloaded IP2Location CSV database"
            fi
          else
            echo "✓ MaxMind database ready: $(ls -lh GeoLite2-City.mmdb)"
          fi
      
      - name: Install Python dependencies
        run: |
          pip install requests maxminddb-geolite2 geoip2 maxminddb ipaddress beautifulsoup4
      
      - name: Fetch Proton VPN server data
        run: |
          echo "Fetching Proton VPN server data..."
          
          # Configure curl with better SSL handling and required headers
          export CURL_OPTS="--tlsv1.2 --max-time 30 --retry 3 --retry-delay 2"
          
          # Try multiple Proton VPN data sources with proper headers/parameters
          echo "Fetching from Proton VPN API with required headers..."
          
          # Add required headers for Proton APIs
          curl -s $CURL_OPTS -L \
            -H "x-pm-appversion: 4.0.0" \
            -H "User-Agent: ProtonVPN/4.0.0" \
            "https://api.protonmail.ch/vpn/logicals" > proton_logicals.json || echo "Failed to fetch logicals API"
          echo "Logicals API: $(wc -c < proton_logicals.json) bytes"
          
          # Try with Platform parameter for config
          curl -s $CURL_OPTS -L \
            -H "x-pm-appversion: 4.0.0" \
            -H "User-Agent: ProtonVPN/4.0.0" \
            "https://api.protonmail.ch/vpn/config?Platform=linux" > proton_config.json || echo "Failed to fetch config API"
          echo "Config API: $(wc -c < proton_config.json) bytes"
          
          # Try servers endpoint with headers
          curl -s $CURL_OPTS -L \
            -H "x-pm-appversion: 4.0.0" \
            -H "User-Agent: ProtonVPN/4.0.0" \
            "https://api.protonmail.ch/vpn/servers" > proton_servers.json || echo "Failed to fetch servers API"
          echo "Servers API: $(wc -c < proton_servers.json) bytes"
          
          # Try alternative newer API endpoints
          echo "Trying newer API endpoints..."
          curl -s $CURL_OPTS -L \
            -H "x-pm-appversion: 4.0.0" \
            -H "User-Agent: ProtonVPN/4.0.0" \
            "https://api.protonvpn.ch/vpn/logicals" > proton_new_logicals.json || echo "Failed to fetch from new endpoint"
          echo "New logicals: $(wc -c < proton_new_logicals.json) bytes"
          
          # Try the public server list endpoint (no auth required)
          echo "Trying public endpoints..."
          curl -s $CURL_OPTS -L "https://api.protonvpn.ch/vpn/servers" > proton_public_servers.json || echo "Failed to fetch public servers"
          echo "Public servers: $(wc -c < proton_public_servers.json) bytes"
          
          # Try fetching from ProtonVPN's public server list page
          echo "Trying to extract from public sources..."
          curl -s $CURL_OPTS -L "https://protonvpn.com/api/vpn/servers" > proton_web_servers.json || echo "Failed to fetch web servers"
          echo "Web servers: $(wc -c < proton_web_servers.json) bytes"
          
          # Check which sources have valid JSON data
          echo ""
          echo "=== Data Source Status ==="
          for file in proton_logicals.json proton_config.json proton_servers.json proton_new_logicals.json proton_public_servers.json proton_web_servers.json; do
            if [ -f "$file" ] && [ -s "$file" ]; then
              if python3 -c "import json; json.load(open('$file'))" 2>/dev/null; then
                echo "✓ $file: Valid JSON ($(wc -c < "$file") bytes)"
                # Check if it contains server data
                if python3 -c "import json; data=json.load(open('$file')); exit(0 if ('LogicalServers' in data or 'Servers' in data or isinstance(data, list)) else 1)" 2>/dev/null; then
                  echo "  → Contains server data"
                else
                  echo "  → No server data found"
                fi
              else
                echo "✗ $file: Invalid JSON or error page"
                head -3 "$file" 2>/dev/null || echo "  (empty file)"
              fi
            else
              echo "✗ $file: Failed or empty"
            fi
          done
      
      - name: Process and enrich Proton VPN data with MaxMind
        run: |
          python3 -c "
          import json
          import csv
          import ipaddress
          import os
          import socket
          from datetime import datetime, timezone
          
          # Helper function to validate IP
          def is_valid_ip(ip):
              try:
                  ipaddress.ip_address(ip)
                  return True
              except ValueError:
                  return False
          
          # Helper function to resolve hostname to IP
          def resolve_hostname(hostname):
              try:
                  return socket.gethostbyname(hostname)
              except:
                  return None
          
          # Load MaxMind database
          def load_maxmind_db():
              try:
                  import maxminddb
                  if os.path.exists('GeoLite2-City.mmdb'):
                      reader = maxminddb.open_database('GeoLite2-City.mmdb')
                      print('✓ Loaded MaxMind GeoLite2 database')
                      return reader
                  else:
                      print('MaxMind database not found')
                      return None
              except ImportError:
                  try:
                      # Try alternative import
                      import geoip2.database
                      if os.path.exists('GeoLite2-City.mmdb'):
                          reader = geoip2.database.Reader('GeoLite2-City.mmdb')
                          print('✓ Loaded MaxMind database with geoip2')
                          return reader
                  except ImportError:
                      print('MaxMind libraries not available')
              return None
          
          # Lookup IP in MaxMind database
          def lookup_maxmind(ip, reader):
              try:
                  if hasattr(reader, 'get'):  # maxminddb reader
                      response = reader.get(ip)
                      if response:
                          country = response.get('country', {})
                          city = response.get('city', {})
                          subdivisions = response.get('subdivisions', [{}])
                          location = response.get('location', {})
                          return {
                              'cc': country.get('iso_code', ''),
                              'cn': country.get('names', {}).get('en', ''),
                              'city': city.get('names', {}).get('en', ''),
                              'region': subdivisions[0].get('names', {}).get('en', '') if subdivisions else '',
                              'latitude': location.get('latitude', ''),
                              'longitude': location.get('longitude', ''),
                              'source': 'MaxMind GeoLite2'
                          }
                  else:  # geoip2 reader
                      response = reader.city(ip)
                      return {
                          'cc': response.country.iso_code or '',
                          'cn': response.country.name or '',
                          'city': response.city.name or '',
                          'region': response.subdivisions.most_specific.name or '',
                          'latitude': float(response.location.latitude) if response.location.latitude else '',
                          'longitude': float(response.location.longitude) if response.location.longitude else '',
                          'source': 'MaxMind GeoLite2'
                      }
              except Exception as e:
                  # IP not found in database is normal, not an error
                  pass
              return None
          
          # Load IP2Location CSV fallback
          def load_ip2location_csv():
              ip_ranges = []
              try:
                  if os.path.exists('ip2location.csv'):
                      with open('ip2location.csv', 'r') as f:
                          reader = csv.reader(f)
                          for row in reader:
                              if len(row) >= 6 and row[0].isdigit():
                                  try:
                                      ip_from = int(row[0])
                                      ip_to = int(row[1])
                                      country_code = row[2].strip('\\\"')
                                      country_name = row[3].strip('\\\"')
                                      region = row[4].strip('\\\"') if len(row) > 4 else ''
                                      city = row[5].strip('\\\"') if len(row) > 5 else ''
                                      if country_code and country_code != '-':
                                          ip_ranges.append({
                                              'from': ip_from,
                                              'to': ip_to,
                                              'cc': country_code,
                                              'cn': country_name,
                                              'region': region,
                                              'city': city
                                          })
                                  except (ValueError, IndexError):
                                      continue
                      print(f'✓ Loaded {len(ip_ranges)} IP ranges from IP2Location CSV')
              except Exception as e:
                  print(f'Could not load IP2Location CSV: {e}')
              return ip_ranges
          
          # Convert IP to integer for range lookup
          def ip_to_int(ip):
              parts = ip.split('.')
              return (int(parts[0]) << 24) + (int(parts[1]) << 16) + (int(parts[2]) << 8) + int(parts[3])
          
          # Lookup IP in CSV database
          def lookup_ip_in_csv(ip, ip_ranges):
              try:
                  ip_int = ip_to_int(ip)
                  for range_data in ip_ranges:
                      if range_data['from'] <= ip_int <= range_data['to']:
                          return {
                              'cc': range_data['cc'],
                              'cn': range_data['cn'],
                              'city': range_data['city'],
                              'region': range_data['region'],
                              'latitude': '',
                              'longitude': '',
                              'source': 'IP2Location CSV'
                          }
              except:
                  pass
              return None
          
          # Load MaxMind and IP2Location databases
          maxmind_reader = load_maxmind_db()
          ip2location_ranges = load_ip2location_csv()
          
          # Try to load Proton VPN data from web scraping
          proton_servers = []
          
          print('Loading Proton VPN server data from web scraping...')
          
          try:
              if os.path.exists('proton_scraped_servers.json') and os.path.getsize('proton_scraped_servers.json') > 0:
                  with open('proton_scraped_servers.json', 'r') as f:
                      scraped_servers = json.load(f)
                  
                  print(f'Processing {len(scraped_servers)} servers from web scraping...')
                  
                  for server in scraped_servers:
                      if isinstance(server, dict) and 'ip' in server:
                          proton_servers.append({
                              'server_name': server.get('server_name', ''),
                              'server_domain': server.get('hostname', ''),
                              'exit_ip': server.get('ip', ''),
                              'entry_ip': '',
                              'target_ip': server.get('ip', ''),
                              'country_code': server.get('country_code', ''),
                              'city': '',
                              'features': '',
                              'tier': '',
                              'load': '',
                              'source_file': 'proton_scraped_servers.json'
                          })
                  
                  print(f'Converted {len(proton_servers)} scraped servers to processing format')
              else:
                  print('No web scraping data found')
                  
          except Exception as e:
              print(f'Error processing scraped servers: {e}')
          
          if not proton_servers:
              print('❌ No Proton VPN server data found from any API endpoint.')
              print('Available data sources returned:')
              for source_file in data_sources:
                  if os.path.exists(source_file):
                      size = os.path.getsize(source_file)
                      print(f'  {source_file}: {size} bytes')
                      if size > 0:
                          try:
                              with open(source_file, 'r') as f:
                                  content = f.read(200)  # First 200 chars
                              print(f'    Content preview: {content[:100]}...')
                          except:
                              pass
              
              print('Unable to proceed without valid server data.')
              exit(1)
          
          # Enrich servers with geolocation data
          enriched_servers = []
          maxmind_count = 0
          ip2location_count = 0
          api_count = 0
          no_data_count = 0
          
          print(f'Processing {len(proton_servers)} Proton VPN servers...')
          
          for i, server in enumerate(proton_servers):
              if i % 100 == 0:
                  print(f'Processed {i}/{len(proton_servers)} servers')
              
              target_ip = server.get('target_ip', '')
              if not target_ip:
                  continue
              
              geo_data = None
              
              # Try MaxMind database first
              if maxmind_reader and is_valid_ip(target_ip):
                  geo_data = lookup_maxmind(target_ip, maxmind_reader)
                  if geo_data:
                      maxmind_count += 1
              
              # Try IP2Location CSV as fallback
              if not geo_data and ip2location_ranges and '.' in target_ip:  # IPv4 only
                  geo_data = lookup_ip_in_csv(target_ip, ip2location_ranges)
                  if geo_data:
                      ip2location_count += 1
              
              # Create enriched server entry
              if geo_data:
                  # Use MaxMind/IP2Location data, fall back to API data
                  enriched_server = {
                      'server_name': server.get('server_name', ''),
                      'server_domain': server.get('server_domain', ''),
                      'exit_ip': server.get('exit_ip', ''),
                      'entry_ip': server.get('entry_ip', ''),
                      'target_ip': target_ip,
                      'country_code': geo_data.get('cc') or server.get('country_code', ''),
                      'country_name': geo_data.get('cn') or '',
                      'region': geo_data.get('region') or '',
                      'city': geo_data.get('city') or server.get('city', ''),
                      'latitude': geo_data.get('latitude', ''),
                      'longitude': geo_data.get('longitude', ''),
                      'features': server.get('features', ''),
                      'tier': server.get('tier', ''),
                      'load': server.get('load', ''),
                      'api_country': server.get('country_code', ''),
                      'api_city': server.get('city', ''),
                      'geo_source': geo_data.get('source', 'Unknown'),
                      'source_file': server.get('source_file', 'reference')
                  }
              else:
                  # Use only API data
                  api_count += 1
                  enriched_server = {
                      'server_name': server.get('server_name', ''),
                      'server_domain': server.get('server_domain', ''),
                      'exit_ip': server.get('exit_ip', ''),
                      'entry_ip': server.get('entry_ip', ''),
                      'target_ip': target_ip,
                      'country_code': server.get('country_code', ''),
                      'country_name': '',
                      'region': '',
                      'city': server.get('city', ''),
                      'latitude': '',
                      'longitude': '',
                      'features': server.get('features', ''),
                      'tier': server.get('tier', ''),
                      'load': server.get('load', ''),
                      'api_country': server.get('country_code', ''),
                      'api_city': server.get('city', ''),
                      'geo_source': 'Proton API',
                      'source_file': server.get('source_file', 'reference')
                  }
              
              enriched_servers.append(enriched_server)
          
          # Calculate statistics
          total_servers = len(enriched_servers)
          
          # Count by geo source
          source_counts = {}
          for server in enriched_servers:
              source = server['geo_source']
              source_counts[source] = source_counts.get(source, 0) + 1
          
          # Count by country
          country_counts = {}
          for server in enriched_servers:
              country = server['country_code']
              if country:
                  country_counts[country] = country_counts.get(country, 0) + 1
          
          print('')
          print('Processing summary:')
          for source, count in source_counts.items():
              print(f'  {source}: {count} servers')
          print(f'Total processed: {total_servers} servers')
          
          # Create enriched CSV
          with open('ExternalData/ProtonVPNServers.csv', 'w', newline='') as f:
              writer = csv.writer(f)
              writer.writerow([
                  'ServerName', 'ServerDomain', 'ExitIP', 'EntryIP', 'TargetIP',
                  'CountryCode', 'CountryName', 'Region', 'City', 
                  'Latitude', 'Longitude', 'Features', 'Tier', 'Load',
                  'APICountry', 'APICity', 'GeoSource', 'SourceFile'
              ])
              for server in enriched_servers:
                  writer.writerow([
                      server['server_name'],
                      server['server_domain'],
                      server['exit_ip'],
                      server['entry_ip'],
                      server['target_ip'],
                      server['country_code'],
                      server['country_name'],
                      server['region'],
                      server['city'],
                      server['latitude'],
                      server['longitude'],
                      server['features'],
                      server['tier'],
                      server['load'],
                      server['api_country'],
                      server['api_city'],
                      server['geo_source'],
                      server['source_file']
                  ])
          
          # Create enriched JSON
          data = {
              'metadata': {
                  'source': 'Proton VPN API with MaxMind GeoLite2 enrichment',
                  'updated': datetime.now(timezone.utc).isoformat(),
                  'total_servers': total_servers,
                  'processing_method': 'API + Local database lookup',
                  'geo_source_breakdown': source_counts,
                  'country_breakdown': dict(sorted(country_counts.items(), key=lambda x: x[1], reverse=True)[:20])
              },
              'ProtonVPNServers': enriched_servers
          }
          
          with open('ExternalData/ProtonVPNServers.json', 'w') as f:
              json.dump(data, f, indent=2)
          
          print('')
          print(f'Created enriched data: {total_servers} Proton VPN servers')
          print('Processing completed successfully!')
          
          # Close database connection
          if maxmind_reader:
              try:
                  maxmind_reader.close()
              except:
                  pass
                  
          "
      
      - name: Generate statistics
        run: |
          echo "=== Proton VPN Server Statistics ==="
          echo "Total servers: $(tail -n +2 ExternalData/ProtonVPNServers.csv | wc -l)"
          
          echo ""
          echo "Top 20 countries by server count:"
          tail -n +2 ExternalData/ProtonVPNServers.csv | cut -d',' -f6 | grep -v '^$' | sort | uniq -c | sort -nr | head -20
          
          echo ""
          echo "Geolocation sources breakdown:"
          tail -n +2 ExternalData/ProtonVPNServers.csv | cut -d',' -f17 | sort | uniq -c | sort -nr
          
          echo ""
          echo "Top 10 cities by server count:"
          tail -n +2 ExternalData/ProtonVPNServers.csv | cut -d',' -f9 | grep -v '^$' | sort | uniq -c | sort -nr | head -10
          
          echo ""
          echo "Server tiers breakdown:"
          tail -n +2 ExternalData/ProtonVPNServers.csv | cut -d',' -f13 | sort | uniq -c | sort -nr
          
          echo ""
          echo "Data sources breakdown:"
          tail -n +2 ExternalData/ProtonVPNServers.csv | cut -d',' -f18 | sort | uniq -c | sort -nr
          
          echo ""
          echo "Sample servers with high load:"
          tail -n +2 ExternalData/ProtonVPNServers.csv | sort -t',' -k14 -nr | head -5 | cut -d',' -f1,6,9,14
      
      - name: Cleanup temporary files
        run: |
          rm -f proton_*.json
          rm -f GeoLite2-City.* *.tar.gz ip2location.csv
      
      - name: Commit and push changes
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "Update Proton VPN servers with MaxMind geolocation enrichment - $(date -u +%Y-%m-%d)"
          file_pattern: 'ExternalData/ProtonVPNServers.*'
          commit_user_name: 'Proton VPN Data Updater'
          commit_user_email: 'actions@github.com'
          push_options: '--force-with-lease'
