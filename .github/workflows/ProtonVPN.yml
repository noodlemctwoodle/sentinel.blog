name: Scrape Proton VPN Server Data

on:
  workflow_dispatch:
  schedule:
    # Run daily at 2 AM UTC
    - cron: "0 2 * * *"

jobs:
  scrape-protonvpn-servers:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Create directory if not exists
        run: mkdir -p ExternalData
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install Python dependencies
        run: |
          pip install requests beautifulsoup4 lxml html5lib
      
      - name: Scrape Proton VPN server data
        run: |
          python3 << 'EOF'
          import requests
          import json
          import re
          from bs4 import BeautifulSoup
          from datetime import datetime, timezone
          import time
          
          def fetch_page_with_retry(url, max_retries=3):
              """Fetch page with retry logic and proper headers"""
              headers = {
                  'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                  'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                  'Accept-Language': 'en-US,en;q=0.5',
                  'Accept-Encoding': 'gzip, deflate',
                  'Connection': 'keep-alive',
                  'Upgrade-Insecure-Requests': '1',
              }
              
              for attempt in range(max_retries):
                  try:
                      print(f"Fetching {url} (attempt {attempt + 1}/{max_retries})")
                      response = requests.get(url, headers=headers, timeout=30)
                      response.raise_for_status()
                      return response.text
                  except requests.RequestException as e:
                      print(f"Attempt {attempt + 1} failed: {e}")
                      if attempt < max_retries - 1:
                          time.sleep(5)  # Wait before retry
                      else:
                          raise
          
          def parse_server_count(text):
              """Extract server count from text like '268 servers | 5 cities'"""
              if not text:
                  return 0, 0
              
              # Match patterns like "268 servers | 5 cities" or "1 server | 1 city"
              server_match = re.search(r'(\d+)\s+servers?', text)
              city_match = re.search(r'(\d+)\s+cit(?:y|ies)', text)
              
              server_count = int(server_match.group(1)) if server_match else 0
              city_count = int(city_match.group(1)) if city_match else 0
              
              return server_count, city_count
          
          def parse_features(features_list):
              """Parse features from the bullet point list"""
              features = []
              for item in features_list:
                  text = item.get_text(strip=True)
                  if text and text not in ['Adblocker (NetShield)', 'Up to 10Gbps connectivity']:
                      features.append(text)
              return features
          
          def scrape_plus_servers(html_content):
              """Scrape Plus servers data from the HTML content"""
              soup = BeautifulSoup(html_content, 'html.parser')
              servers = []
              
              # Find the Plus section
              plus_section = soup.find('div', {'id': 'plus'}) or soup.find('h2', string='Plus')
              if plus_section:
                  plus_section = plus_section.find_parent() if plus_section.find_parent() else plus_section
              
              if not plus_section:
                  # Try to find the section by looking for the Plus heading
                  for heading in soup.find_all(['h1', 'h2', 'h3']):
                      if 'Plus' in heading.get_text():
                          plus_section = heading.find_parent()
                          break
              
              if plus_section:
                  # Look for country entries in the Plus section
                  country_blocks = plus_section.find_all(['div', 'section'], class_=lambda x: x and ('country' in x.lower() or 'server' in x.lower()))
                  
                  if not country_blocks:
                      # Alternative approach: find all text that looks like country entries
                      text = plus_section.get_text()
                      lines = text.split('\n')
                      
                      current_country = None
                      current_features = []
                      
                      for line in lines:
                          line = line.strip()
                          if not line:
                              continue
                          
                          # Check if this line contains server count info
                          if re.search(r'\d+\s+servers?\s*\|\s*\d+\s+cit(?:y|ies)', line):
                              if current_country:
                                  server_count, city_count = parse_server_count(line)
                                  servers.append({
                                      'country': current_country,
                                      'server_count': server_count,
                                      'city_count': city_count,
                                      'features': current_features.copy(),
                                      'server_type': 'Plus'
                                  })
                              current_features = []
                              current_country = None
                          # Check if this line is a feature
                          elif line.startswith('- ') or line.startswith('•'):
                              feature = line.lstrip('- •').strip()
                              if feature and feature not in ['Adblocker (NetShield)', 'Up to 10Gbps connectivity']:
                                  current_features.append(feature)
                          # Check if this might be a country name
                          elif not line.startswith('-') and not line.startswith('•') and len(line) > 2:
                              # Country names typically don't contain numbers or special chars
                              if not re.search(r'\d+|[<>{}]', line) and line not in ['Plus', 'Streaming support', 'NetShield Ad-blocker', 'P2P/BitTorrent support', 'Tor over VPN', 'Fastest speed (up to 10 Gbps)']:
                                  current_country = line
              
              print(f"Found {len(servers)} Plus servers")
              return servers
          
          def scrape_secure_core_servers(html_content):
              """Scrape Secure Core servers data from the HTML content"""
              soup = BeautifulSoup(html_content, 'html.parser')
              servers = []
              
              # Find the Secure Core section
              secure_core_section = soup.find('div', {'id': 'secure-core'}) or soup.find('h2', string='Secure Core')
              if secure_core_section:
                  secure_core_section = secure_core_section.find_parent() if secure_core_section.find_parent() else secure_core_section
              
              if not secure_core_section:
                  # Try to find the section by looking for the Secure Core heading
                  for heading in soup.find_all(['h1', 'h2', 'h3']):
                      if 'Secure Core' in heading.get_text():
                          secure_core_section = heading.find_parent()
                          break
              
              if secure_core_section:
                  # Look for country entries in the Secure Core section
                  text = secure_core_section.get_text()
                  lines = text.split('\n')
                  
                  current_country = None
                  current_features = []
                  
                  for line in lines:
                      line = line.strip()
                      if not line:
                          continue
                      
                      # Check if this line contains server count info
                      if re.search(r'\d+\s+servers?$', line):
                          if current_country:
                              server_match = re.search(r'(\d+)\s+servers?', line)
                              server_count = int(server_match.group(1)) if server_match else 0
                              servers.append({
                                  'country': current_country,
                                  'server_count': server_count,
                                  'city_count': 1,  # Secure Core typically has 1 city per country
                                  'features': current_features.copy() + ['Secure Core'],
                                  'server_type': 'Secure Core'
                              })
                          current_features = []
                          current_country = None
                      # Check if this line is a feature
                      elif line.startswith('- ') or line.startswith('•'):
                          feature = line.lstrip('- •').strip()
                          if feature and feature not in ['Adblocker (NetShield)']:
                              current_features.append(feature)
                      # Check if this might be a country name
                      elif not line.startswith('-') and not line.startswith('•') and len(line) > 2:
                          # Country names typically don't contain numbers or special chars
                          if not re.search(r'\d+|[<>{}]', line) and line not in ['Secure Core', 'Learn More']:
                              current_country = line
              
              print(f"Found {len(servers)} Secure Core servers")
              return servers
          
          def main():
              print("Starting Proton VPN server data scraping...")
              
              # URLs to scrape
              urls = [
                  'https://protonvpn.com/vpn-servers#plus',
                  'https://protonvpn.com/vpn-servers#secure-core'
              ]
              
              all_servers = []
              
              for url in urls:
                  try:
                      html_content = fetch_page_with_retry(url)
                      
                      if '#plus' in url:
                          servers = scrape_plus_servers(html_content)
                      else:
                          servers = scrape_secure_core_servers(html_content)
                      
                      all_servers.extend(servers)
                      
                  except Exception as e:
                      print(f"Error scraping {url}: {e}")
                      continue
              
              if not all_servers:
                  print("No server data found. Attempting alternative parsing...")
                  
                  # Try scraping the main page without fragments
                  try:
                      html_content = fetch_page_with_retry('https://protonvpn.com/vpn-servers')
                      soup = BeautifulSoup(html_content, 'html.parser')
                      
                      # Extract all text and look for patterns
                      text = soup.get_text()
                      
                      # Pattern matching for countries with server counts
                      country_pattern = r'([A-Za-z\s&\-\'()]+)(?:\n|\s+)-\s*(?:.*?\n)*?(\d+)\s+servers?\s*\|\s*(\d+)\s+cit(?:y|ies)'
                      matches = re.findall(country_pattern, text, re.MULTILINE)
                      
                      for match in matches:
                          country, server_count, city_count = match
                          country = country.strip()
                          if country and len(country) < 50:  # Reasonable country name length
                              all_servers.append({
                                  'country': country,
                                  'server_count': int(server_count),
                                  'city_count': int(city_count),
                                  'features': [],
                                  'server_type': 'Plus'
                              })
                      
                      print(f"Found {len(all_servers)} servers using pattern matching")
                      
                  except Exception as e:
                      print(f"Alternative parsing also failed: {e}")
              
              # Create the final data structure
              total_plus_servers = sum(s['server_count'] for s in all_servers if s['server_type'] == 'Plus')
              total_secure_core_servers = sum(s['server_count'] for s in all_servers if s['server_type'] == 'Secure Core')
              
              data = {
                  'metadata': {
                      'source': 'Proton VPN Website Scraper',
                      'scraped_at': datetime.now(timezone.utc).isoformat(),
                      'total_plus_servers': total_plus_servers,
                      'total_secure_core_servers': total_secure_core_servers,
                      'total_countries': len(set(s['country'] for s in all_servers)),
                      'scraping_method': 'BeautifulSoup HTML parsing'
                  },
                  'servers': all_servers
              }
              
              # Save to JSON
              with open('ExternalData/ProtonVPNServers.json', 'w') as f:
                  json.dump(data, f, indent=2)
              
              # Create CSV
              import csv
              with open('ExternalData/ProtonVPNServers.csv', 'w', newline='', encoding='utf-8') as f:
                  writer = csv.writer(f)
                  writer.writerow(['Country', 'ServerType', 'ServerCount', 'CityCount', 'Features'])
                  
                  for server in all_servers:
                      writer.writerow([
                          server['country'],
                          server['server_type'],
                          server['server_count'],
                          server['city_count'],
                          '; '.join(server['features'])
                      ])
              
              print(f"\n=== Scraping Complete ===")
              print(f"Total servers found: {len(all_servers)}")
              print(f"Plus servers: {total_plus_servers} across {len([s for s in all_servers if s['server_type'] == 'Plus'])} countries")
              print(f"Secure Core servers: {total_secure_core_servers} across {len([s for s in all_servers if s['server_type'] == 'Secure Core'])} countries")
              print(f"Data saved to ExternalData/ProtonVPNServers.json and .csv")
          
          if __name__ == "__main__":
              main()
          EOF
      
      - name: Generate statistics
        run: |
          echo "=== Proton VPN Server Statistics ==="
          
          if [ -f "ExternalData/ProtonVPNServers.csv" ]; then
            echo "Total entries: $(tail -n +2 ExternalData/ProtonVPNServers.csv | wc -l)"
            
            echo ""
            echo "Server type breakdown:"
            tail -n +2 ExternalData/ProtonVPNServers.csv | cut -d',' -f2 | sort | uniq -c | sort -nr
            
            echo ""
            echo "Top 20 countries by server count:"
            tail -n +2 ExternalData/ProtonVPNServers.csv | awk -F',' '{print $3 "," $1}' | sort -t',' -k1 -nr | head -20
            
            echo ""
            echo "Countries with Secure Core servers:"
            grep "Secure Core" ExternalData/ProtonVPNServers.csv | cut -d',' -f1 | sort
            
            echo ""
            echo "Total server count by type:"
            tail -n +2 ExternalData/ProtonVPNServers.csv | awk -F',' '{
              if ($2 == "Plus") plus += $3;
              if ($2 == "Secure Core") secure += $3;
            } END {
              print "Plus servers: " plus;
              print "Secure Core servers: " secure;
              print "Total: " (plus + secure)
            }'
          else
            echo "No CSV file found - scraping may have failed"
          fi
      
      - name: Validate scraped data
        run: |
          python3 << 'EOF'
          import json
          import sys
          
          try:
              with open('ExternalData/ProtonVPNServers.json', 'r') as f:
                  data = json.load(f)
              
              servers = data.get('servers', [])
              metadata = data.get('metadata', {})
              
              print(f"Validation Results:")
              print(f"- Valid JSON: ✓")
              print(f"- Total entries: {len(servers)}")
              print(f"- Metadata present: {'✓' if metadata else '✗'}")
              
              if len(servers) == 0:
                  print("❌ No servers found - scraping failed")
                  sys.exit(1)
              
              # Check for required fields
              required_fields = ['country', 'server_count', 'server_type']
              for i, server in enumerate(servers[:5]):  # Check first 5
                  missing = [field for field in required_fields if field not in server]
                  if missing:
                      print(f"❌ Missing fields in entry {i}: {missing}")
                      sys.exit(1)
              
              print(f"✓ Data validation passed")
              print(f"Sample entry: {servers[0] if servers else 'None'}")
              
          except Exception as e:
              print(f"❌ Validation failed: {e}")
              sys.exit(1)
          EOF
      
      - name: Commit and push changes
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "Update Proton VPN servers data - $(date -u '+%Y-%m-%d %H:%M UTC')"
          file_pattern: 'ExternalData/ProtonVPNServers.*'
          commit_user_name: 'Proton VPN Scraper'
          commit_user_email: 'actions@github.com'
          push_options: '--force-with-lease'
      
      - name: Create summary
        run: |
          echo "## Proton VPN Server Scraping Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f "ExternalData/ProtonVPNServers.json" ]; then
            echo "✅ **Scraping Successful**" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            # Get metadata from JSON
            python3 -c "
          import json
          with open('ExternalData/ProtonVPNServers.json', 'r') as f:
              data = json.load(f)
          
          metadata = data['metadata']
          servers = data['servers']
          
          print(f'- **Total Countries:** {metadata.get(\"total_countries\", \"N/A\")}')
          print(f'- **Plus Servers:** {metadata.get(\"total_plus_servers\", \"N/A\")}')
          print(f'- **Secure Core Servers:** {metadata.get(\"total_secure_core_servers\", \"N/A\")}')
          print(f'- **Scraped At:** {metadata.get(\"scraped_at\", \"N/A\")}')
          print(f'- **Total Entries:** {len(servers)}')
          " >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ **Scraping Failed**" >> $GITHUB_STEP_SUMMARY
            echo "No data files were created." >> $GITHUB_STEP_SUMMARY
          fi
