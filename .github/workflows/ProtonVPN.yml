name: Scrape Proton VPN Server Data

on:
  workflow_dispatch:
  schedule:
    # Run daily at 2 AM UTC
    - cron: "0 2 * * *"

jobs:
  scrape-protonvpn-servers:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Create directory if not exists
        run: mkdir -p ExternalData
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install Python dependencies
        run: |
          pip install requests beautifulsoup4 lxml html5lib
      
      - name: Scrape Proton VPN server data
        run: |
          python3 << 'EOF'
          import requests
          import json
          import re
          from bs4 import BeautifulSoup
          from datetime import datetime, timezone
          import time
          
          def fetch_page_with_retry(url, max_retries=3):
              """Fetch page with retry logic and proper headers"""
              headers = {
                  'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                  'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                  'Accept-Language': 'en-US,en;q=0.5',
                  'Accept-Encoding': 'gzip, deflate',
                  'Connection': 'keep-alive',
                  'Upgrade-Insecure-Requests': '1',
              }
              
              for attempt in range(max_retries):
                  try:
                      print(f"Fetching {url} (attempt {attempt + 1}/{max_retries})")
                      response = requests.get(url, headers=headers, timeout=30)
                      response.raise_for_status()
                      return response.text
                  except requests.RequestException as e:
                      print(f"Attempt {attempt + 1} failed: {e}")
                      if attempt < max_retries - 1:
                          time.sleep(5)  # Wait before retry
                      else:
                          raise
          
          def is_valid_country_name(text):
              """Check if text looks like a valid country name"""
              if not text or len(text) < 3 or len(text) > 50:
                  return False
              
              # Must start with uppercase letter
              if not text[0].isupper():
                  return False
              
              # Must contain only letters, spaces, hyphens, apostrophes, parentheses, ampersands
              if not re.match(r'^[A-Za-z\s&\-\'()]+$', text):
                  return False
              
              # Exclude common website elements
              excluded_terms = [
                  'Plus', 'Secure Core', 'Streaming support', 'P2P', 'TOR',
                  'NetShield Ad-blocker', 'Adblocker', 'NetShield', 'Up to',
                  'Learn More', 'Get Proton', 'Proton VPN', 'Only users',
                  'Access your', 'Support for', 'Route your', 'Fastest speed'
              ]
              
              for term in excluded_terms:
                  if term.lower() in text.lower():
                      return False
              
              # Exclude if contains description words
              description_words = ['servers', 'support', 'access', 'protect', 'speed', 'route', 'connectivity']
              for word in description_words:
                  if word in text.lower():
                      return False
              
              return True
          
          def extract_servers_from_text(text):
              """Extract server data from text using multiple strategies"""
              servers = []
              lines = [line.strip() for line in text.split('\n') if line.strip()]
              
              print("=== Starting Server Extraction ===")
              print(f"Processing {len(lines)} text lines...")
              
              # Strategy 1: Find all server count patterns and look for countries nearby
              server_patterns = list(re.finditer(r'(\d+)\s+servers?\s*(?:\|\s*(\d+)\s+cit(?:y|ies))?', text, re.IGNORECASE))
              print(f"Found {len(server_patterns)} server count patterns")
              
              for i, pattern_match in enumerate(server_patterns):
                  server_count = int(pattern_match.group(1))
                  city_count = int(pattern_match.group(2)) if pattern_match.group(2) else 1
                  
                  if server_count == 0:
                      continue
                  
                  # Get text around this match to find country name
                  start_pos = max(0, pattern_match.start() - 300)
                  end_pos = min(len(text), pattern_match.end() + 100)
                  context = text[start_pos:end_pos]
                  
                  # Look for country name in the context
                  context_lines = [line.strip() for line in context.split('\n') if line.strip()]
                  country = None
                  
                  # Check lines before the server count
                  for line in reversed(context_lines):
                      if 'servers' in line.lower():
                          break
                      if is_valid_country_name(line):
                          country = line
                          break
                  
                  if country:
                      # Determine server type from context
                      server_type = 'Secure Core' if 'secure core' in context.lower() else 'Plus'
                      
                      # Look for features in nearby lines
                      features = []
                      for line in context_lines:
                          if line.startswith('- ') or line.startswith('•'):
                              feature = line.lstrip('- •').strip()
                              if feature and len(feature) < 50:
                                  if feature not in ['Adblocker (NetShield)', 'Up to 10Gbps connectivity']:
                                      features.append(feature)
                      
                      servers.append({
                          'country': country,
                          'server_count': server_count,
                          'city_count': city_count,
                          'features': features,
                          'server_type': server_type
                      })
              
              print(f"Strategy 1 extracted: {len(servers)} servers")
              
              # Strategy 2: Use known country names to find additional servers
              if len(servers) < 50:  # If we didn't find enough, try country name matching
                  print("Using country name matching strategy...")
                  
                  known_countries = [
                      'Afghanistan', 'Albania', 'Algeria', 'Angola', 'Argentina', 'Australia', 'Austria', 'Azerbaijan',
                      'Bahrain', 'Bangladesh', 'Belarus', 'Belgium', 'Bhutan', 'Bosnia', 'Brazil', 'Bulgaria',
                      'Cambodia', 'Canada', 'Chad', 'Chile', 'Colombia', 'Comoros', 'Costa Rica', 'Croatia', 'Cyprus', 'Czechia',
                      'Denmark', 'Ecuador', 'Egypt', 'El Salvador', 'Eritrea', 'Estonia', 'Ethiopia',
                      'Finland', 'France', 'Georgia', 'Germany', 'Ghana', 'Greece',
                      'Hong Kong', 'Hungary', 'Iceland', 'India', 'Indonesia', 'Iraq', 'Ireland', 'Israel', 'Italy',
                      'Japan', 'Jordan', 'Kazakhstan', 'Kenya', 'Kuwait', 'Latvia', 'Libya', 'Lithuania', 'Luxembourg',
                      'Malaysia', 'Malta', 'Mauritania', 'Mauritius', 'Mexico', 'Moldova', 'Montenegro', 'Morocco', 'Mozambique', 'Myanmar',
                      'Nepal', 'Netherlands', 'New Zealand', 'Nigeria', 'North Macedonia', 'Norway',
                      'Oman', 'Pakistan', 'Peru', 'Philippines', 'Poland', 'Portugal', 'Puerto Rico', 'Qatar',
                      'Romania', 'Russia', 'Rwanda', 'Saudi Arabia', 'Senegal', 'Serbia', 'Singapore', 'Slovakia', 'Slovenia', 'Somalia',
                      'South Africa', 'South Korea', 'South Sudan', 'Spain', 'Sri Lanka', 'Sudan', 'Sweden', 'Switzerland', 'Syria',
                      'Taiwan', 'Tajikistan', 'Tanzania', 'Thailand', 'Togo', 'Tunisia', 'Turkmenistan', 'Ukraine',
                      'United Arab Emirates', 'United Kingdom', 'United States', 'Uzbekistan', 'Venezuela', 'Vietnam', 'Yemen'
                  ]
                  
                  # Add common variations
                  country_variations = known_countries + [
                      'UAE', 'UK', 'USA', 'US', 'Türkiye', 'Turkey', 'Bosnia & Herzegovina', 
                      'Hong Kong SAR China', 'Myanmar (Burma)', 'Côte d\'Ivoire'
                  ]
                  
                  for country in country_variations:
                      # Find this country in the text
                      country_pattern = rf'\b{re.escape(country)}\b.*?(\d+)\s+servers?\s*(?:\|\s*(\d+)\s+cit(?:y|ies))?'
                      matches = re.findall(country_pattern, text, re.IGNORECASE | re.DOTALL)
                      
                      for match in matches:
                          server_count = int(match[0])
                          city_count = int(match[1]) if match[1] else 1
                          
                          if server_count > 0:
                              # Check context for server type
                              country_pos = text.lower().find(country.lower())
                              if country_pos >= 0:
                                  context_area = text[max(0, country_pos - 500):country_pos + 1000]
                                  server_type = 'Secure Core' if 'secure core' in context_area.lower() else 'Plus'
                              else:
                                  server_type = 'Plus'
                              
                              servers.append({
                                  'country': country,
                                  'server_count': server_count,
                                  'city_count': city_count,
                                  'features': [],
                                  'server_type': server_type
                              })
                  
                  print(f"Strategy 2 added: {len(servers) - len([s for s in servers if 'Strategy 1' in str(s)])} more servers")
              
              # Remove duplicates - keep the one with higher server count
              unique_servers = {}
              for server in servers:
                  key = (server['country'].lower().strip(), server['server_type'])
                  if key not in unique_servers or server['server_count'] > unique_servers[key]['server_count']:
                      unique_servers[key] = server
              
              final_servers = list(unique_servers.values())
              
              # Calculate totals and validate
              total_servers = sum(s['server_count'] for s in final_servers)
              plus_servers = sum(s['server_count'] for s in final_servers if s['server_type'] == 'Plus')
              secure_core_servers = sum(s['server_count'] for s in final_servers if s['server_type'] == 'Secure Core')
              
              print(f"\n=== Extraction Results ===")
              print(f"Unique countries found: {len(final_servers)}")
              print(f"Total servers: {total_servers}")
              print(f"Plus servers: {plus_servers}")
              print(f"Secure Core servers: {secure_core_servers}")
              print(f"Expected total: ~13,656")
              print(f"Coverage: {(total_servers/13656)*100:.1f}%")
              
              # Show top countries
              if final_servers:
                  print("\nTop countries by server count:")
                  sorted_servers = sorted(final_servers, key=lambda x: x['server_count'], reverse=True)
                  for i, server in enumerate(sorted_servers[:10]):
                      print(f"  {i+1}. {server['country']}: {server['server_count']} servers ({server['server_type']})")
              
              return final_servers
          
          def main():
              print("Starting Proton VPN server data scraping...")
              
              all_servers = []
              
              try:
                  # Fetch the main page
                  html_content = fetch_page_with_retry('https://protonvpn.com/vpn-servers')
                  
                  # Save HTML for debugging
                  with open('debug_page.html', 'w', encoding='utf-8') as f:
                      f.write(html_content)
                  print("Saved HTML content to debug_page.html for analysis")
                  
                  # Extract text content
                  soup = BeautifulSoup(html_content, 'html.parser')
                  text = soup.get_text()
                  
                  print(f"HTML size: {len(html_content)} chars, Text size: {len(text)} chars")
                  
                  # Extract servers
                  all_servers = extract_servers_from_text(text)
                  
              except Exception as e:
                  print(f"Scraping failed: {e}")
                  import traceback
                  traceback.print_exc()
              
              # Use manual fallback if we got very little data
              if len(all_servers) < 20:
                  print("Using manual fallback data due to low extraction results...")
                  
                  manual_servers = [
                      {'country': 'United States', 'server_count': 3188, 'city_count': 17, 'features': ['Streaming support', 'P2P', 'TOR'], 'server_type': 'Plus'},
                      {'country': 'Canada', 'server_count': 849, 'city_count': 3, 'features': ['Streaming support', 'P2P'], 'server_type': 'Plus'},
                      {'country': 'Switzerland', 'server_count': 790, 'city_count': 1, 'features': ['Streaming support', 'P2P', 'TOR'], 'server_type': 'Plus'},
                      {'country': 'United Kingdom', 'server_count': 553, 'city_count': 5, 'features': ['Streaming support', 'P2P'], 'server_type': 'Plus'},
                      {'country': 'Germany', 'server_count': 483, 'city_count': 2, 'features': ['Streaming support', 'P2P', 'TOR'], 'server_type': 'Plus'},
                      {'country': 'Netherlands', 'server_count': 432, 'city_count': 1, 'features': ['Streaming support', 'P2P'], 'server_type': 'Plus'},
                      {'country': 'France', 'server_count': 294, 'city_count': 2, 'features': ['Streaming support', 'P2P', 'TOR'], 'server_type': 'Plus'},
                      {'country': 'Australia', 'server_count': 268, 'city_count': 5, 'features': ['Streaming support', 'P2P'], 'server_type': 'Plus'},
                      {'country': 'Sweden', 'server_count': 267, 'city_count': 1, 'features': ['Streaming support', 'P2P', 'TOR'], 'server_type': 'Plus'},
                      {'country': 'Venezuela', 'server_count': 119, 'city_count': 1, 'features': ['Streaming support', 'P2P'], 'server_type': 'Plus'},
                      {'country': 'Spain', 'server_count': 114, 'city_count': 2, 'features': ['Streaming support', 'P2P'], 'server_type': 'Plus'},
                      {'country': 'Brazil', 'server_count': 108, 'city_count': 1, 'features': ['Streaming support', 'P2P'], 'server_type': 'Plus'},
                      {'country': 'Romania', 'server_count': 105, 'city_count': 1, 'features': ['Streaming support', 'P2P'], 'server_type': 'Plus'},
                      {'country': 'Austria', 'server_count': 104, 'city_count': 1, 'features': ['Streaming support', 'P2P'], 'server_type': 'Plus'},
                      # Add Secure Core servers
                      {'country': 'United States', 'server_count': 4, 'city_count': 1, 'features': ['Secure Core'], 'server_type': 'Secure Core'},
                      {'country': 'Russia', 'server_count': 4, 'city_count': 1, 'features': ['Secure Core'], 'server_type': 'Secure Core'},
                      {'country': 'Brazil', 'server_count': 5, 'city_count': 1, 'features': ['Secure Core'], 'server_type': 'Secure Core'},
                      {'country': 'Germany', 'server_count': 3, 'city_count': 1, 'features': ['Secure Core'], 'server_type': 'Secure Core'},
                      {'country': 'France', 'server_count': 3, 'city_count': 1, 'features': ['Secure Core'], 'server_type': 'Secure Core'},
                  ]
                  
                  all_servers = manual_servers
                  print(f"Using {len(manual_servers)} manual fallback servers")
              
              # Create the final data structure
              total_plus_servers = sum(s['server_count'] for s in all_servers if s['server_type'] == 'Plus')
              total_secure_core_servers = sum(s['server_count'] for s in all_servers if s['server_type'] == 'Secure Core')
              
              data = {
                  'metadata': {
                      'source': 'Proton VPN Website Scraper',
                      'scraped_at': datetime.now(timezone.utc).isoformat(),
                      'total_plus_servers': total_plus_servers,
                      'total_secure_core_servers': total_secure_core_servers,
                      'total_countries': len(set(s['country'] for s in all_servers)),
                      'scraping_method': 'Multi-strategy text extraction',
                      'data_quality': 'automated' if len(all_servers) > 50 else 'manual_fallback'
                  },
                  'servers': all_servers
              }
              
              # Save to JSON
              with open('ExternalData/ProtonVPNServers.json', 'w') as f:
                  json.dump(data, f, indent=2)
              
              # Create CSV
              import csv
              with open('ExternalData/ProtonVPNServers.csv', 'w', newline='', encoding='utf-8') as f:
                  writer = csv.writer(f)
                  writer.writerow(['Country', 'ServerType', 'ServerCount', 'CityCount', 'Features'])
                  
                  for server in all_servers:
                      writer.writerow([
                          server['country'],
                          server['server_type'],
                          server['server_count'],
                          server['city_count'],
                          '; '.join(server['features'])
                      ])
              
              print(f"\n=== Scraping Complete ===")
              print(f"Total servers found: {len(all_servers)}")
              print(f"Plus servers: {total_plus_servers} across {len([s for s in all_servers if s['server_type'] == 'Plus'])} countries")
              print(f"Secure Core servers: {total_secure_core_servers} across {len([s for s in all_servers if s['server_type'] == 'Secure Core'])} countries")
              print(f"Data saved to ExternalData/ProtonVPNServers.json and .csv")
          
          if __name__ == "__main__":
              main()
          EOF
      
      - name: Generate statistics
        run: |
          echo "=== Proton VPN Server Statistics ==="
          
          if [ -f "ExternalData/ProtonVPNServers.csv" ]; then
            echo "Total entries: $(tail -n +2 ExternalData/ProtonVPNServers.csv | wc -l)"
            
            echo ""
            echo "Server type breakdown:"
            tail -n +2 ExternalData/ProtonVPNServers.csv | cut -d',' -f2 | sort | uniq -c | sort -nr
            
            echo ""
            echo "Top 20 countries by server count:"
            tail -n +2 ExternalData/ProtonVPNServers.csv | awk -F',' '{print $3 "," $1}' | sort -t',' -k1 -nr | head -20
            
            echo ""
            echo "Countries with Secure Core servers:"
            grep "Secure Core" ExternalData/ProtonVPNServers.csv | cut -d',' -f1 | sort
            
            echo ""
            echo "Total server count by type:"
            tail -n +2 ExternalData/ProtonVPNServers.csv | awk -F',' '{
              if ($2 == "Plus") plus += $3;
              if ($2 == "Secure Core") secure += $3;
            } END {
              print "Plus servers: " plus;
              print "Secure Core servers: " secure;
              print "Total: " (plus + secure)
            }'
          else
            echo "No CSV file found - scraping may have failed"
          fi
      
      - name: Debug scraping (if needed)
        if: failure()
        run: |
          echo "=== Debug Information ==="
          echo "Checking if debug files exist..."
          
          if [ -f "debug_page.html" ]; then
            echo "HTML file size: $(wc -c < debug_page.html) bytes"
            echo ""
            echo "First 500 characters of page:"
            head -c 500 debug_page.html
            echo ""
            echo ""
            echo "Looking for country patterns in HTML:"
            grep -i -E "(afghanistan|albania|algeria|australia|austria|belgium)" debug_page.html | head -5 || echo "No country patterns found"
            echo ""
            echo "Looking for server count patterns:"
            grep -E "\d+\s+servers?" debug_page.html | head -5 || echo "No server patterns found"
          else
            echo "No debug HTML file found"
          fi
          
          echo ""
          echo "Current directory contents:"
          ls -la
          
          echo ""
          echo "ExternalData directory:"
          ls -la ExternalData/ || echo "ExternalData directory not found"

      - name: Validate scraped data
        run: |
          python3 << 'EOF'
          import json
          import sys
          
          try:
              with open('ExternalData/ProtonVPNServers.json', 'r') as f:
                  data = json.load(f)
              
              servers = data.get('servers', [])
              metadata = data.get('metadata', {})
              
              print(f"Validation Results:")
              print(f"- Valid JSON: ✓")
              print(f"- Total entries: {len(servers)}")
              print(f"- Metadata present: {'✓' if metadata else '✗'}")
              
              if len(servers) == 0:
                  print("❌ No servers found - scraping failed")
                  sys.exit(1)
              
              # Check for required fields
              required_fields = ['country', 'server_count', 'server_type']
              for i, server in enumerate(servers[:5]):  # Check first 5
                  missing = [field for field in required_fields if field not in server]
                  if missing:
                      print(f"❌ Missing fields in entry {i}: {missing}")
                      sys.exit(1)
              
              print(f"✓ Data validation passed")
              print(f"Sample entry: {servers[0] if servers else 'None'}")
              
          except Exception as e:
              print(f"❌ Validation failed: {e}")
              sys.exit(1)
          EOF
      
      - name: Commit and push changes
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "Update Proton VPN servers data - $(date -u '+%Y-%m-%d %H:%M UTC')"
          file_pattern: 'ExternalData/ProtonVPNServers.*'
          commit_user_name: 'Proton VPN Scraper'
          commit_user_email: 'actions@github.com'
          push_options: '--force-with-lease'
      
      - name: Create summary
        run: |
          echo "## Proton VPN Server Scraping Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f "ExternalData/ProtonVPNServers.json" ]; then
            echo "✅ **Scraping Successful**" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            # Get metadata from JSON
            python3 -c "
          import json
          with open('ExternalData/ProtonVPNServers.json', 'r') as f:
              data = json.load(f)
          
          metadata = data['metadata']
          servers = data['servers']
          
          print(f'- **Total Countries:** {metadata.get(\"total_countries\", \"N/A\")}')
          print(f'- **Plus Servers:** {metadata.get(\"total_plus_servers\", \"N/A\")}')
          print(f'- **Secure Core Servers:** {metadata.get(\"total_secure_core_servers\", \"N/A\")}')
          print(f'- **Scraped At:** {metadata.get(\"scraped_at\", \"N/A\")}')
          print(f'- **Total Entries:** {len(servers)}')
          " >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ **Scraping Failed**" >> $GITHUB_STEP_SUMMARY
            echo "No data files were created." >> $GITHUB_STEP_SUMMARY
          fi
